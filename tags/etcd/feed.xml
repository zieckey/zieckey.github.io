<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Etcd on CodeG Blog</title>
    <link>http://blog.codeg.cn/tags/etcd/</link>
    <description>Recent content in Etcd on CodeG Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>Copyright (c) 2015. All rights reserved.</copyright>
    <lastBuildDate>Tue, 14 Mar 2017 18:13:00 +0000</lastBuildDate>
    <atom:link href="http://blog.codeg.cn/tags/etcd/feed/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>evpp与muduo吞吐量对比</title>
      <link>http://blog.codeg.cn/post/blog/2017-03-14-the-throughput-benchmark-test-of-evpp-vs-muduo/</link>
      <pubDate>Tue, 14 Mar 2017 18:13:00 +0000</pubDate>
      
      <guid>http://blog.codeg.cn/post/blog/2017-03-14-the-throughput-benchmark-test-of-evpp-vs-muduo/</guid>
      <description>

&lt;h3 id=&#34;简介:251aeffb3a5bb5eef7cdbb7cd5914ca7&#34;&gt;简介&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;是最近几年中国开源界里产生的优秀作品。它是由业内大牛陈硕实现的。详细介绍，请参考其博客介绍&lt;a href=&#34;http://blog.csdn.net/solstice/article/details/5848547&#34;&gt;http://blog.csdn.net/solstice/article/details/5848547&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;本次测试是参考陈硕的博客文章&lt;a href=&#34;http://blog.csdn.net/solstice/article/details/5864889&#34;&gt;muduo与libevent2吞吐量对比&lt;/a&gt;，该文章的结论是：&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;吞吐量平均比&lt;a href=&#34;https://github.com/libevent/libevent&#34;&gt;libevent2&lt;/a&gt;高 &lt;strong&gt;18%&lt;/strong&gt; 以上。&lt;/p&gt;

&lt;p&gt;由于&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;本身是基于&lt;a href=&#34;https://github.com/libevent/libevent&#34;&gt;libevent2&lt;/a&gt;实现的，因此我们希望将&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;和&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;放到一起做一次全面的性能测试。本文是关于这两个库在吞吐量方面的测试。&lt;/p&gt;

&lt;h3 id=&#34;测试对象:251aeffb3a5bb5eef7cdbb7cd5914ca7&#34;&gt;测试对象&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Qihoo360/evpp/archive/v0.2.4.zip&#34;&gt;evpp-v0.2.4&lt;/a&gt; based on libevent-2.0.21&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chenshuo/muduo/archive/v1.0.9.zip&#34;&gt;muduo-v1.0.9&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;测试环境:251aeffb3a5bb5eef7cdbb7cd5914ca7&#34;&gt;测试环境&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Linux CentOS 6.2, 2.6.32-220.7.1.el6.x86_64&lt;/li&gt;
&lt;li&gt;Intel&amp;reg; Xeon&amp;reg; CPU E5-2630 v2 @ 2.60GHz&lt;/li&gt;
&lt;li&gt;gcc version 4.8.2 20140120 (Red Hat 4.8.2-15) (GCC)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;测试方法:251aeffb3a5bb5eef7cdbb7cd5914ca7&#34;&gt;测试方法&lt;/h3&gt;

&lt;p&gt;依据 boost.asio 性能测试 &lt;a href=&#34;http://think-async.com/Asio/LinuxPerformanceImprovements&#34;&gt;http://think-async.com/Asio/LinuxPerformanceImprovements&lt;/a&gt; 的办法，用 ping pong 协议来测试吞吐量。&lt;/p&gt;

&lt;p&gt;简单地说，ping pong 协议是客户端和服务器都实现 echo 协议。当 TCP 连接建立时，客户端向服务器发送一些数据，服务器会 echo 回这些数据，然后客户端再 echo 回服务器。这些数据就会像乒乓球一样在客户端和服务器之间来回传送，直到有一方断开连接为止。这是用来测试吞吐量的常用办法。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;的测试代码在软件包内的路径为 &lt;code&gt;examples/pingpong/&lt;/code&gt;，代码如&lt;a href=&#34;https://github.com/chenshuo/muduo/tree/master/examples/pingpong&#34;&gt;https://github.com/chenshuo/muduo/tree/master/examples/pingpong&lt;/a&gt;所示。并使用&lt;code&gt;BUILD_TYPE=release ./build.sh&lt;/code&gt;方式编译&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;的优化版本。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;的测试代码在软件包内的路径为&lt;code&gt;benchmark/throughput/evpp&lt;/code&gt;，代码如&lt;a href=&#34;https://github.com/Qihoo360/evpp/tree/master/benchmark/throughput/evpp&#34;&gt;https://github.com/Qihoo360/evpp/tree/master/benchmark/throughput/evpp&lt;/a&gt;所示。并使用 &lt;code&gt;tools&lt;/code&gt;目录下的&lt;code&gt;benchmark-build.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;我们做了下面两项测试：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;单线程测试，测试并发连接数为 1/10/100/1000/10000 时，消息大小分别为 4096 8192 81920 409600 时的吞吐量&lt;/li&gt;
&lt;li&gt;多线程测试，并发连接数为 100 或 1000，服务器和客户端的线程数同时设为 2/3/4/6/8，ping pong 消息的大小为 16k bytes。测试用的 shell 脚本可从&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;的源码包中找到。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;单线程测试结果数据:251aeffb3a5bb5eef7cdbb7cd5914ca7&#34;&gt;单线程测试结果数据&lt;/h3&gt;

&lt;p&gt;最终测试结论如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在吞吐量方面的性能总体来说，比较接近，各擅胜场&lt;/li&gt;
&lt;li&gt;在单个消息较大时（&amp;gt;81K)，&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;比&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;整体上更快&lt;/li&gt;
&lt;li&gt;在单个消息不太大时，并发数小于1000时，&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;占优&lt;/li&gt;
&lt;li&gt;在单个消息不太大时，并发数大于1000时，&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;占优&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;测试中，单个消息较大时，&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;比&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;整体上更快的结论，我们认为是与&lt;code&gt;Buffer&lt;/code&gt;类的设计实现有关。&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;的&lt;code&gt;Buffer&lt;/code&gt;类是自己人肉实现的内存管理，而&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;的&lt;code&gt;Buffer&lt;/code&gt;类的底层是用&lt;code&gt;std::vector&amp;lt;char&amp;gt;&lt;/code&gt;实现的，我们推测&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;的这个实现性能方面稍差。本次吞吐量测试中，主要的开销是网络IO事件的触发回调和数据读写，当消息size不太大时，网络IO的事件触发耗费CPU更多；当消息size较大时，数据的读写和拷贝占用更多CPU。当然这只是一个推测，后面如果有时间或大家感兴趣，可以自行验证两个库的&lt;code&gt;Buffer&lt;/code&gt;类的操作性能。&lt;/p&gt;

&lt;p&gt;这个测试结果进一步推断，&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;比&lt;a href=&#34;https://github.com/libevent/libevent&#34;&gt;libevent2&lt;/a&gt;更快（因为&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;吞吐量平均比&lt;a href=&#34;https://github.com/libevent/libevent&#34;&gt;libevent2&lt;/a&gt;高 &lt;strong&gt;18%&lt;/strong&gt; 以上），表面上看不符合逻辑，因为&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;的底层就是&lt;a href=&#34;https://github.com/libevent/libevent&#34;&gt;libevent2&lt;/a&gt;，但仔细分析发现，&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;只是用了&lt;a href=&#34;https://github.com/libevent/libevent&#34;&gt;libevent2&lt;/a&gt;核心的事件循环，并没有用&lt;a href=&#34;https://github.com/libevent/libevent&#34;&gt;libevent2&lt;/a&gt;中的&lt;code&gt;evbuffer&lt;/code&gt;相关类和函数来读写网络数据，而是借鉴&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;和&lt;a href=&#34;https://golang.org&#34;&gt;Golang&lt;/a&gt;实现了自己独立的[Buffer]类来读写网络数据。&lt;/p&gt;

&lt;p&gt;下面是具体的测试数据和图表。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Message Size&lt;/th&gt;
&lt;th&gt;1 connection&lt;/th&gt;
&lt;th&gt;10 connections&lt;/th&gt;
&lt;th&gt;100 connections&lt;/th&gt;
&lt;th&gt;1000 connections&lt;/th&gt;
&lt;th&gt;10000 connections&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;evpp&lt;/td&gt;
&lt;td&gt;4096&lt;/td&gt;
&lt;td&gt;229.274&lt;/td&gt;
&lt;td&gt;631.611&lt;/td&gt;
&lt;td&gt;671.219&lt;/td&gt;
&lt;td&gt;495.566&lt;/td&gt;
&lt;td&gt;366.071&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;muduo&lt;/td&gt;
&lt;td&gt;4096&lt;/td&gt;
&lt;td&gt;222.117&lt;/td&gt;
&lt;td&gt;609.152&lt;/td&gt;
&lt;td&gt;631.119&lt;/td&gt;
&lt;td&gt;514.235&lt;/td&gt;
&lt;td&gt;365.959&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;evpp&lt;/td&gt;
&lt;td&gt;8192&lt;/td&gt;
&lt;td&gt;394.162&lt;/td&gt;
&lt;td&gt;1079.67&lt;/td&gt;
&lt;td&gt;1127.09&lt;/td&gt;
&lt;td&gt;786.706&lt;/td&gt;
&lt;td&gt;645.866&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;muduo&lt;/td&gt;
&lt;td&gt;8192&lt;/td&gt;
&lt;td&gt;393.683&lt;/td&gt;
&lt;td&gt;1064.43&lt;/td&gt;
&lt;td&gt;1103.02&lt;/td&gt;
&lt;td&gt;815.269&lt;/td&gt;
&lt;td&gt;670.503&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;evpp&lt;/td&gt;
&lt;td&gt;81920&lt;/td&gt;
&lt;td&gt;1565.22&lt;/td&gt;
&lt;td&gt;2079.77&lt;/td&gt;
&lt;td&gt;1464.16&lt;/td&gt;
&lt;td&gt;1323.09&lt;/td&gt;
&lt;td&gt;1297.18&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;muduo&lt;/td&gt;
&lt;td&gt;81920&lt;/td&gt;
&lt;td&gt;1567.959&lt;/td&gt;
&lt;td&gt;2180.467&lt;/td&gt;
&lt;td&gt;1432.009&lt;/td&gt;
&lt;td&gt;1267.181&lt;/td&gt;
&lt;td&gt;1159.278&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;evpp&lt;/td&gt;
&lt;td&gt;409600&lt;/td&gt;
&lt;td&gt;1950.79&lt;/td&gt;
&lt;td&gt;2363.68&lt;/td&gt;
&lt;td&gt;1528.97&lt;/td&gt;
&lt;td&gt;1290.17&lt;/td&gt;
&lt;td&gt;1039.96&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;muduo&lt;/td&gt;
&lt;td&gt;409600&lt;/td&gt;
&lt;td&gt;1887.057&lt;/td&gt;
&lt;td&gt;2213.813&lt;/td&gt;
&lt;td&gt;1305.899&lt;/td&gt;
&lt;td&gt;1131.383&lt;/td&gt;
&lt;td&gt;1043.612&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;我们用&lt;a href=&#34;https://github.com/zieckey/gochart&#34;&gt;https://github.com/zieckey/gochart&lt;/a&gt;这个图表绘制工具将上述数据绘制为图表。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zieckey/resources/master/evpp/benchmark/throughput/1thread-4096-evpp-vs-muduo.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/zieckey/resources/master/evpp/benchmark/throughput/1thread-8192-evpp-vs-muduo.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/zieckey/resources/master/evpp/benchmark/throughput/1thread-81920-evpp-vs-muduo.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/zieckey/resources/master/evpp/benchmark/throughput/1thread-409600-evpp-vs-muduo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;多线程测试结果:251aeffb3a5bb5eef7cdbb7cd5914ca7&#34;&gt;多线程测试结果&lt;/h3&gt;

&lt;p&gt;测试结论如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在多线程场景下，&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;和&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;两个库在吞吐量方面，的性能整体上来看没有明显区别，分阶段分别领先&lt;/li&gt;
&lt;li&gt;100并发连接比1000并发连接测试，两个库的吞吐量都明显的高得多&lt;/li&gt;
&lt;li&gt;在100并发连接测试下，随着线程数的增长，吞吐量基本上是线性增长。&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;库在中段领先于&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;，但在前期和后期又弱于&lt;a href=&#34;https://github.com/Qihoo360/evpp&#34;&gt;evpp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;在1000并发连接测试下，随着线程数的增长，前期基本上是线性增长，后期增长乏力。&lt;a href=&#34;https://github.com/chenshuo/muduo&#34;&gt;muduo&lt;/a&gt;库这方面表现尤其明显&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zieckey/resources/master/evpp/benchmark/throughput/multi-thread-evpp-vs-muduo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;[Buffer]:&lt;a href=&#34;https://github.com/Qihoo360/evpp/blob/master/evpp/buffer.h&#34;&gt;https://github.com/Qihoo360/evpp/blob/master/evpp/buffer.h&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用Golang利用ectd实现一个分布式锁</title>
      <link>http://blog.codeg.cn/post/blog/2016-02-24-distrubute-lock-over-etcd/</link>
      <pubDate>Wed, 24 Feb 2016 20:43:00 +0000</pubDate>
      
      <guid>http://blog.codeg.cn/post/blog/2016-02-24-distrubute-lock-over-etcd/</guid>
      <description>

&lt;p&gt;&lt;code&gt;etcd&lt;/code&gt;是随着&lt;code&gt;CoreOS&lt;/code&gt;项目一起成长起来的，随着Golang和CoreOS等项目在开源社区日益火热，
&lt;code&gt;etcd&lt;/code&gt;作为一个高可用、强一致性的分布式Key-Value存储系统被越来越多的开发人员关注和使用。&lt;/p&gt;

&lt;p&gt;这篇&lt;a href=&#34;http://www.infoq.com/cn/articles/etcd-interpretation-application-scenario-implement-principle&#34;&gt;文章&lt;/a&gt;全方位介绍了etcd的应用场景，这里简单摘要如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;服务发现（Service Discovery）&lt;/li&gt;
&lt;li&gt;消息发布与订阅&lt;/li&gt;
&lt;li&gt;负载均衡&lt;/li&gt;
&lt;li&gt;分布式通知与协调&lt;/li&gt;
&lt;li&gt;分布式锁&lt;/li&gt;
&lt;li&gt;分布式队列&lt;/li&gt;
&lt;li&gt;集群监控与Leader竞选&lt;/li&gt;
&lt;li&gt;为什么用etcd而不用ZooKeeper&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文重点介绍如何利用&lt;code&gt;ectd&lt;/code&gt;实现一个分布式锁。
锁的概念大家都熟悉，当我们希望某一事件在同一时间点只有一个线程(goroutine)在做，或者某一个资源在同一时间点只有一个服务能访问，这个时候我们就需要用到锁。
例如我们要实现一个分布式的id生成器，多台服务器之间的协调就非常麻烦。分布式锁就正好派上用场。&lt;/p&gt;

&lt;p&gt;其基本实现原理为：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在ectd系统里创建一个key&lt;/li&gt;
&lt;li&gt;如果创建失败，key存在，则监听该key的变化事件，直到该key被删除，回到1&lt;/li&gt;
&lt;li&gt;如果创建成功，则认为我获得了锁&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;具体代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package etcdsync

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;io&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;sync&amp;quot;
	&amp;quot;time&amp;quot;

	&amp;quot;github.com/coreos/etcd/client&amp;quot;
	&amp;quot;github.com/coreos/etcd/Godeps/_workspace/src/golang.org/x/net/context&amp;quot;
)

const (
	defaultTTL = 60
	defaultTry = 3
	deleteAction = &amp;quot;delete&amp;quot;
	expireAction = &amp;quot;expire&amp;quot;
)

// A Mutex is a mutual exclusion lock which is distributed across a cluster.
type Mutex struct {
	key    string
	id     string // The identity of the caller
	client client.Client
	kapi   client.KeysAPI
	ctx    context.Context
	ttl    time.Duration
	mutex  *sync.Mutex
	logger io.Writer
}

// New creates a Mutex with the given key which must be the same
// across the cluster nodes.
// machines are the ectd cluster addresses
func New(key string, ttl int, machines []string) *Mutex {
	cfg := client.Config{
		Endpoints:               machines,
		Transport:               client.DefaultTransport,
		HeaderTimeoutPerRequest: time.Second,
	}

	c, err := client.New(cfg)
	if err != nil {
		return nil
	}

	hostname, err := os.Hostname()
	if err != nil {
		return nil
	}

	if len(key) == 0 || len(machines) == 0 {
		return nil
	}

	if key[0] != &#39;/&#39; {
		key = &amp;quot;/&amp;quot; + key
	}

	if ttl &amp;lt; 1 {
		ttl = defaultTTL
	}

	return &amp;amp;Mutex{
		key:    key,
		id:     fmt.Sprintf(&amp;quot;%v-%v-%v&amp;quot;, hostname, os.Getpid(), time.Now().Format(&amp;quot;20060102-15:04:05.999999999&amp;quot;)),
		client: c,
		kapi:   client.NewKeysAPI(c),
		ctx: context.TODO(),
		ttl: time.Second * time.Duration(ttl),
		mutex:  new(sync.Mutex),
	}
}

// Lock locks m.
// If the lock is already in use, the calling goroutine
// blocks until the mutex is available.
func (m *Mutex) Lock() (err error) {
	m.mutex.Lock()
	for try := 1; try &amp;lt;= defaultTry; try++ {
		if m.lock() == nil {
			return nil
		}
		
		m.debug(&amp;quot;Lock node %v ERROR %v&amp;quot;, m.key, err)
		if try &amp;lt; defaultTry {
			m.debug(&amp;quot;Try to lock node %v again&amp;quot;, m.key, err)
		}
	}
	return err
}

func (m *Mutex) lock() (err error) {
	m.debug(&amp;quot;Trying to create a node : key=%v&amp;quot;, m.key)
	setOptions := &amp;amp;client.SetOptions{
		PrevExist:client.PrevNoExist,
		TTL:      m.ttl,
	}
	resp, err := m.kapi.Set(m.ctx, m.key, m.id, setOptions)
	if err == nil {
		m.debug(&amp;quot;Create node %v OK [%q]&amp;quot;, m.key, resp)
		return nil
	}
	m.debug(&amp;quot;Create node %v failed [%v]&amp;quot;, m.key, err)
	e, ok := err.(client.Error)
	if !ok {
		return err
	}

	if e.Code != client.ErrorCodeNodeExist {
		return err
	}

	// Get the already node&#39;s value.
	resp, err = m.kapi.Get(m.ctx, m.key, nil)
	if err != nil {
		return err
	}
	m.debug(&amp;quot;Get node %v OK&amp;quot;, m.key)
	watcherOptions := &amp;amp;client.WatcherOptions{
		AfterIndex : resp.Index,
		Recursive:false,
	}
	watcher := m.kapi.Watcher(m.key, watcherOptions)
	for {
		m.debug(&amp;quot;Watching %v ...&amp;quot;, m.key)
		resp, err = watcher.Next(m.ctx)
		if err != nil {
			return err
		}

		m.debug(&amp;quot;Received an event : %q&amp;quot;, resp)
		if resp.Action == deleteAction || resp.Action == expireAction {
			return nil
		}
	}

}

// Unlock unlocks m.
// It is a run-time error if m is not locked on entry to Unlock.
//
// A locked Mutex is not associated with a particular goroutine.
// It is allowed for one goroutine to lock a Mutex and then
// arrange for another goroutine to unlock it.
func (m *Mutex) Unlock() (err error) {
	defer m.mutex.Unlock()
	for i := 1; i &amp;lt;= defaultTry; i++ {
		var resp *client.Response
		resp, err = m.kapi.Delete(m.ctx, m.key, nil)
		if err == nil {
			m.debug(&amp;quot;Delete %v OK&amp;quot;, m.key)
			return nil
		}
		m.debug(&amp;quot;Delete %v falied: %q&amp;quot;, m.key, resp)
		e, ok := err.(client.Error)
		if ok &amp;amp;&amp;amp; e.Code == client.ErrorCodeKeyNotFound {
			return nil
		}
	}
	return err
}

func (m *Mutex) debug(format string, v ...interface{}) {
	if m.logger != nil {
		m.logger.Write([]byte(m.id))
		m.logger.Write([]byte(&amp;quot; &amp;quot;))
		m.logger.Write([]byte(fmt.Sprintf(format, v...)))
		m.logger.Write([]byte(&amp;quot;\n&amp;quot;))
	}
}

func (m *Mutex) SetDebugLogger(w io.Writer) {
	m.logger = w
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其实类似的实现有很多，但目前都已经过时，使用的都是被官方标记为&lt;code&gt;deprecated&lt;/code&gt;的项目。且大部分接口都不如上述代码简单。
使用上，跟Golang官方sync包的Mutex接口非常类似，先&lt;code&gt;New()&lt;/code&gt;，然后调用&lt;code&gt;Lock()&lt;/code&gt;，使用完后调用&lt;code&gt;Unlock()&lt;/code&gt;，就三个接口，就是这么简单。示例代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;github.com/zieckey/etcdsync&amp;quot;
	&amp;quot;log&amp;quot;
)

func main() {
	//etcdsync.SetDebug(true)
	log.SetFlags(log.Ldate|log.Ltime|log.Lshortfile)
	m := etcdsync.New(&amp;quot;/etcdsync&amp;quot;, &amp;quot;123&amp;quot;, []string{&amp;quot;http://127.0.0.1:2379&amp;quot;})
	if m == nil {
		log.Printf(&amp;quot;etcdsync.NewMutex failed&amp;quot;)
	}
	err := m.Lock()
	if err != nil {
		log.Printf(&amp;quot;etcdsync.Lock failed&amp;quot;)
	} else {
		log.Printf(&amp;quot;etcdsync.Lock OK&amp;quot;)
	}

	log.Printf(&amp;quot;Get the lock. Do something here.&amp;quot;)

	err = m.Unlock()
	if err != nil {
		log.Printf(&amp;quot;etcdsync.Unlock failed&amp;quot;)
	} else {
		log.Printf(&amp;quot;etcdsync.Unlock OK&amp;quot;)
	}
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;参考:cc4776a5974d000eb2d6ff22e22b2c04&#34;&gt;参考&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/zieckey/etcdsync&#34;&gt;etcdsync项目地址&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;ectd项目官方地址&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>